{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we will use data from [here](http://data.princeton.edu/wws509/datasets/#salary), in which we want to see whether there's discrimination in professor salaries or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import pandas as pd\n",
    "import theano.tensor as tt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Type\n",
    "\n",
    "This is standard linear regression! To those who have been trained in stats, this shouldn't be too difficult to understand. \n",
    "\n",
    "\n",
    "\n",
    "## Data structure\n",
    "\n",
    "To use it with this model, the data should be structured as such:\n",
    "\n",
    "- Each row is one measurement.\n",
    "- The columns should indicate, at the minimum:\n",
    "    - The explanatory variables, each getting their own column.\n",
    "    - The dependent variable, one column.\n",
    "\n",
    "## Extensions to the model\n",
    "\n",
    "None\n",
    "\n",
    "## Reporting summarized findings\n",
    "\n",
    "Here are examples of how to summarize the findings.\n",
    "\n",
    "> For every unit increase in `explanatory_variable_i`, dependent variable increases by `beta` (95% HPD: [`lower`, `upper`]).\n",
    "\n",
    "## Other notes\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are from [here][datasource], and have the following columns:\n",
    "\n",
    "Independent variables:\n",
    "\n",
    "- `sx`: the biological sex of the professor.\n",
    "    - 0 for male\n",
    "    - 1 for female\n",
    "- `rk`: the rank of the professor.\n",
    "    - 1 for assistant professor\n",
    "    - 2 for associate professor\n",
    "    - 3 for full professor\n",
    "- `dg`: the highest degree attained\n",
    "    - 0 for masters degree\n",
    "    - 1 for doctorate degre\n",
    "- `yd`: years since degree obtained. Essentially an 'experience' term.\n",
    "\n",
    "Dependent variables:\n",
    "\n",
    "- `sl`: annual salary\n",
    "\n",
    "[datasource]: http://data.princeton.edu/wws509/datasets/#salary\n",
    "\n",
    "# Read Data\n",
    "\n",
    "Let's read in the data and do some preprocessing to make all of the data numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "df = pd.read_csv(\"../datasets/professor-salary.txt\")\n",
    "\n",
    "\n",
    "def replace_sx(sex):\n",
    "    \"\"\"\n",
    "    This function codes the biological sex of the professor.\n",
    "    \"\"\"\n",
    "    if sex == \"male\":\n",
    "        return 0\n",
    "    elif sex == \"female\":\n",
    "        return 1\n",
    "\n",
    "\n",
    "def replace_rk(rank):\n",
    "    \"\"\"\n",
    "    This function codes the highest rank attained.\n",
    "    \"\"\"\n",
    "    if rank == \"full\":\n",
    "        return 3\n",
    "    elif rank == \"associate\":\n",
    "        return 2\n",
    "    elif rank == \"assistant\":\n",
    "        return 1\n",
    "\n",
    "\n",
    "def replace_dg(degree):\n",
    "    \"\"\"\n",
    "    This function codes the highest degree earned.\n",
    "    \"\"\"\n",
    "    if degree == \"doctorate\":\n",
    "        return 1\n",
    "    elif degree == \"masters\":\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Perform variable numerical encoding.\n",
    "df[\"sx\"] = df[\"sx\"].apply(lambda x: replace_sx(x))\n",
    "df[\"rk\"] = df[\"rk\"].apply(lambda x: replace_rk(x))\n",
    "df[\"dg\"] = df[\"dg\"].apply(lambda x: replace_dg(x))\n",
    "\n",
    "# Preview the data\n",
    "# import patsy\n",
    "X = df[[\"sx\", \"rk\", \"yr\", \"dg\", \"yd\"]]\n",
    "# X = patsy.dmatrix('sx + rk + yr + dg + yd -1', X, return_type='dataframe')\n",
    "Y = df[\"sl\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "We will perform linear regression on the salary data. Here's some of the modelling choices that go into this.\n",
    "\n",
    "1. Choice of priors:\n",
    "    1. Intercept: Normal distribution. Very wide.\n",
    "    1. Errors: Can only be positive, therefore use HalfNormal distribution, again, very wide.\n",
    "1. Choices for salary likelihood function:\n",
    "    1. The salary is modelled as a linear combination of the independent variables.\n",
    "    1. We assume that the salary is going to be normally distributed around the linear combination of independent variables with the same variance around the expected value.\n",
    "    \n",
    "That is how we get the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# X = pm.floatX(X)\n",
    "# df.columns = ['sx', 'rk', 'yr', 'dg', 'yd', 'sl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano.tensor as tt\n",
    "\n",
    "with pm.Model() as model:\n",
    "\n",
    "    #     intercept = pm.Normal('intercept', mu=0, sd=100**2)\n",
    "    #     error = pm.HalfNormal('error', sd=100**2)\n",
    "\n",
    "    #     b = pm.Normal('betas', mu=0, sd=100**2, shape=(X.shape[1], 1))\n",
    "    #     sl = tt.dot(X, b) + intercept\n",
    "\n",
    "    #     sl_like = pm.Normal('likelihood', mu=sl, sd=error, observed=Y)\n",
    "\n",
    "    pm.glm.linear.GLM(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the recipe above, you'll have a general starting point for linear regressions (and its variants, e.g. poisson regression). The key idea, which you'll see later on, is swapping out the likelihood function.\n",
    "\n",
    "The awesome PyMC3 developers provide also a GLM module that lets you write the above more concisely:\n",
    "\n",
    "```python\n",
    "with pm.Model() as model:\n",
    "    pm.glm.glm('sl ~ sx + rk + yr + dg + yd', df)\n",
    "```\n",
    "\n",
    "However, I have given you the more verbose version, as I want you to see the code at the level of abstraction that will let you flexibly modify the model as you need it.\n",
    "\n",
    "Borrowing shamelessly from Thomas Wiecki, we hit the Inference Button (TM) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = pm.sample(draws=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the traceplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traceplots give us a visual diagnostic on the convergence of the MCMC sampler. The ADVI initialization gets us pretty darn close to the places of highest likelihood. Sampling converges pretty soon after, so let's use a **burn-in** of ~1000 steps and re-check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnin = 1000\n",
    "pm.traceplot(trace[burnin:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should be pretty clear - very good convergence. Let's look at a forestplot of the inferred variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.forestplot(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percs(varname):\n",
    "    return np.round(np.percentile(trace[varname], [2.5, 50, 97.5]), 0)\n",
    "\n",
    "\n",
    "intercept_percs = get_percs(\"Intercept\")\n",
    "sex_percs = get_percs(\"sx\")\n",
    "year_percs = get_percs(\"yr\")\n",
    "rank_percs = get_percs(\"rk\")\n",
    "degree_percs = get_percs(\"dg\")\n",
    "experience_percs = get_percs(\"yd\")\n",
    "sd_percs = get_percs(\"sd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     " intercept_percs[0] ": "10329.0",
     " intercept_percs[2] ": "14516.0",
     "degree_percs[0]": "-1187.0",
     "degree_percs[2]": "1541.0",
     "experience_percs[0]": "-98.0",
     "experience_percs[2]": "159.0",
     "rank_percs[0]": "2525.0",
     "rank_percs[2]": "4803.0",
     "sex_percs[0]": "-1144.0",
     "sex_percs[2]": "1505.0",
     "year_percs[0]": "250.0",
     "year_percs[2]": "618.0"
    }
   },
   "source": [
    "# Interpretation\n",
    "\n",
    "The interpretation here is as such. \n",
    "\n",
    "Given the data on hand, \n",
    "\n",
    "- a professor's baseline salary is in the range of \\${{ intercept_percs[0] }} to \\${{ intercept_percs[2] }} dollars\n",
    "- every increase in rank gives \\${{rank_percs[0]}} to \\${{rank_percs[2]}} dollars increase in salary\n",
    "- females earn \\${{sex_percs[0]}} to \\${{sex_percs[2]}} more dollars than males\n",
    "- every extra year of work earns the professor \\${{year_percs[0]}} to \\${{year_percs[2]}} in salary\n",
    "- having an advanced degree earns the professor \\${{degree_percs[0]}} to \\${{degree_percs[2]}} in salary\n",
    "- every year away from the degree earned earns the professor \\${{experience_percs[0]}} to \\${{experience_percs[2]}} in salary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "Overall, rank and years of work are the best predictors of professor salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian",
   "language": "python",
   "name": "bayesian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "192px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
