{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Models From The Bottom Up, with Python\n",
    "\n",
    "Markov models are a useful class of models for sequential-type of data. Before recurrent neural networks (which can be thought of as an upgraded Markov model) came along, Markov Models and their variants were _the in thing_ for processing time series and biological data. \n",
    "\n",
    "Just recently, I was involved in a project with a colleague, Zach Barry, where we thought the use of autoregressive hidden Markov models (AR-HMMs) might be a useful thing. Apart from our hack session one afternoon, it set off a series of self-study that culminated in this essay. By writing this down for my own memory, my hope is that it gives you a resource to refer back to as well.\n",
    "\n",
    "In this essay, you'll notice that I don't talk about inference (i.e. inferring parameters from data) until the end: this is intentional. As I've learned over the years doing statistical modelling and machine learning, nothing makes sense without first becoming deeply familiar with the \"generative\" story of each model, i.e. the algorithmic steps that let us generate data. It's a very Bayesian-influenced way of thinking that I hope you will become familiar with too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Models (HMMs): What they are, with in mostly plain English and some math\n",
    "\n",
    "The simplest Markov Models assume that we have a _system_ that belongs to one of a finite set of states, and that the _system_ transitions between these states with some probability at each time step $t$, thus generating a sequence of states over time. Let's call these states $S$, where \n",
    "\n",
    "$$S = \\{s_1, s_2, ..., s_n\\}$$\n",
    "\n",
    "To keep things simple, let's stick with unique prime numbers and go with a two-state model:\n",
    "\n",
    "$$S = \\{s_1, s_2, s_3\\}$$\n",
    "\n",
    "They thus generate a sequence of states:\n",
    "\n",
    "$$\\{s_1, s_1, s_1, s_3, s_3, s_3, s_2, s_2, s_3, s_3, s_3, s_3, s_1, ...\\}$$\n",
    "\n",
    "### Initializing a Markov chain\n",
    "\n",
    "First up, we need an **initial state probability matrix**, which tells us what the distribution of initial states will be. Let's call the matrix $p_S$, where the subscript $S$ indicates that it is for the \"states\".\n",
    "\n",
    "$$ p_S = \n",
    "\\begin{pmatrix} p_1 & p_2 & p_3\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Semantically, they allocate the probabilities of starting the sequence at a given state. Without this, we might assume a discrete uniform distribution, which in Python would look like:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "p_init = np.array([1/3., 1/3., 1/3.])\n",
    "```\n",
    "\n",
    "Alternatively, we might assume a fixed starting point, which can be expressed as the $p_S$ array:\n",
    "\n",
    "```python\n",
    "p_init = np.array([0, 1, 0])\n",
    "```\n",
    "\n",
    "Alternatively, we might assign non-zero probabilities to each in a non-uniform fashion:\n",
    "\n",
    "```python\n",
    "# State 0: 0.1 probability\n",
    "# State 1: 0.8 probability\n",
    "# State 2: 0.1 probability\n",
    "p_init = np.array([0.1, 0.8, 0.1])\n",
    "```\n",
    "\n",
    "Let's stick with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "p_init = np.array([0.1, 0.8, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling transitions between states\n",
    "\n",
    "We now need a **transition matrix**. The transition matrix describes the probability of transitioning from one state to another. (The probability of staying in the same state is semantically equivalent to transitioning to the same state.) \n",
    "\n",
    "By convention, transition matrix rows correspond to the state at time $t$, while columns correspond to state at time $t+1$. Hence, row probabilities sum to one, because the probability of transitioning to the next state depends on only the current state, and since all possible states are enumerated, they must sum to one. \n",
    "\n",
    "Let's call the transition matrix $p_T$. The symbol etymology, which usually gets swept under the rug in mathematically-oriented papers, are as follows: \n",
    "\n",
    "- $T$ doesn't refer to time but simply indicates that it is for transitioning states, \n",
    "- $p$ is used because it is a probility matrix.\n",
    "\n",
    "$$ p_T = \n",
    "\\begin{pmatrix}\n",
    "    p_{11} & p_{12} & p_{13}\\\\\n",
    "    p_{21} & p_{22} & p_{23}\\\\\n",
    "    p_{31} & p_{32} & p_{33}\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "In the interest of keeping things brief, I won't illustrate the myriad of scenarios that we can have here. I will, however, remind you that the row probabilities must sum to one.\n",
    "\n",
    "In Python code, we'll initialize a transition probability matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_transition = np.array(\n",
    "    [[0.90, 0.05, 0.05], \n",
    "     [0.01, 0.90, 0.09], \n",
    "     [0.07, 0.03, 0.9]]\n",
    ")\n",
    "p_transition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just to confirm with you that each row sums to one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert p_transition[0, :].sum() == 1\n",
    "assert p_transition[1, :].sum() == 1\n",
    "assert p_transition[2, :].sum() == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equilibrium or Stationary Distribution\n",
    "\n",
    "The stationary or equilibrium distribution of a Markov chain is the distribution of observed states at infinite time. An interesting property is that regardless of what the initial state is, the equilibrium distribution will always be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_init_example = np.array([0.1, 0.8, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_state_t = [p_init_example]\n",
    "p_transition_example = np.array(\n",
    "    [[0.6,  0.2, 0.2], \n",
    "     [0.05, 0.9, 0.05], \n",
    "     [0.1,  0.2, 0.7]]\n",
    ")\n",
    "\n",
    "for i in range(200):  # 200 time steps sorta, kinda, approximates infinite time :)\n",
    "    p_state_t.append(p_state_t[-1] @ p_transition_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.vstack(p_state_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're viewing this notebook on Binder or locally, go ahead and modify the initial state to convince yourself that it doesn't matter what the initial state will be, the final state will always be the same as long as the transition matrix stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_state_t[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, there's also a way to solve for the equilibrium distribution analytically from the transition matrix. This involves solving a linear algebra problem, which we can do using Python. (Credit goes to [this blog post](https://towardsdatascience.com/markov-chain-analysis-and-simulation-using-python-4507cee0b06e) from which I modified the code to fit the variable naming here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equilibrium_distribution(p_transition):\n",
    "    n_states = p_transition.shape[0]\n",
    "    A = np.append(p_transition.T - np.eye(n_states), np.ones(n_states).reshape(1, -1), axis=0)\n",
    "    b = np.transpose(np.array([0] * n_states + [1]))\n",
    "    p_eq = np.linalg.solve(np.transpose(A).dot(A), np.transpose(A).dot(b))\n",
    "    return p_eq\n",
    "\n",
    "print(equilibrium_distribution(p_transition_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a Markov Sequence\n",
    "\n",
    "Generating a Markov sequence means we \"forward\" simulate the chain by:\n",
    "\n",
    "(1) Drawing an initial state from $p_S$ (let's call that $s_{t}$). This is done by drawing from a **multinomial** distribution:\n",
    "\n",
    "$$s_t \\sim Multinomial(1, p_S)$$\n",
    "\n",
    "(2) Drawing the next state by indexing into the transition matrix $p_T$, and drawing a new state based on the Multinomial distribution:\n",
    "\n",
    "$$s_{t+1} \\sim Multinomial(1, p_{T_i})$$\n",
    "\n",
    "where $i$ is the index of the state.\n",
    "\n",
    "In Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multinomial\n",
    "from typing import List\n",
    "\n",
    "def markov_sequence(p_init: np.array, p_transition: np.array, sequence_length: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Generate a Markov sequence based on p_init and p_transition.\n",
    "    \"\"\"\n",
    "    initial_state = list(multinomial.rvs(1, p_init)).index(1)\n",
    "    \n",
    "    states = [initial_state]\n",
    "    for i in range(sequence_length - 1):\n",
    "        p_tr = p_transition[states[-1]]\n",
    "        new_state = list(multinomial.rvs(1, p_tr)).index(1)\n",
    "        states.append(new_state)\n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function in hand, let's generate a sequence of length 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "states = markov_sequence(p_init, p_transition, sequence_length=1000)\n",
    "plt.plot(states)\n",
    "plt.xlabel(\"time step\")\n",
    "plt.ylabel(\"state\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is pretty evident from the transition probabilities, once the Markov chain enters a state, it tends not to move out of it.\n",
    "\n",
    "If you've opened up this notebook in Binder or locally, feel free to modify the transition probabilities and initial state probabilities above to see how the Markov sequence changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emissions: When Markov chains not only produce \"states\", but also observable data\n",
    "\n",
    "So as you've seen above, a Markov chain can produce \"states\". If we are given direct access to the \"states\", then a problem that we may have is inferring the transition probabilities given the states. \n",
    "\n",
    "A more common scenario, however, is that the states are **latent**, i.e. we cannot directly observe them. Instead, the latent states generate data that are given by some distribution conditioned on the state. We call these **Hidden Markov Models**.\n",
    "\n",
    "That all sounds abstract, so let's try to make it more concrete.\n",
    "\n",
    "### Gaussian Emissions: When Markov chains emit Gaussian-distributed data.\n",
    "\n",
    "With a three state model, we might say that the emissions are Gaussian distributed, but the location ($\\mu$) and scale ($\\sigma$) vary based on which state we are in. In the simplest case:\n",
    "\n",
    "1. State 1 gives us data $y_1 \\sim N(\\mu=1, \\sigma=0.2)$\n",
    "1. State 2 gives us data $y_2 \\sim N(\\mu=0, \\sigma=0.5)$\n",
    "1. State 3 gives us data $y_3 \\sim N(\\mu=-1, \\sigma=0.1)$\n",
    "\n",
    "Turns out, we can model this in Python code too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "def gaussian_emissions(states: List[int], mus: List[float], sigmas: List[float]) -> List[float]:\n",
    "    emissions = []\n",
    "    for state in states:\n",
    "        loc = mus[state]\n",
    "        scale = sigmas[state]\n",
    "        e = norm.rvs(loc=loc, scale=scale)\n",
    "        emissions.append(e)\n",
    "    return emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the emissions look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "gaussian_ems = gaussian_emissions(states, mus=[1, 0, -1], sigmas=[0.2, 0.5, 0.1])\n",
    "\n",
    "def plot_emissions(states, emissions):\n",
    "    fig, axes = plt.subplots(figsize=(16, 8), nrows=2, ncols=1, sharex=True)\n",
    "\n",
    "    axes[0].plot(states)\n",
    "    axes[0].set_title(\"States\")\n",
    "    axes[1].plot(emissions)\n",
    "    axes[1].set_title(\"Emissions\")\n",
    "    sns.despine();\n",
    "    \n",
    "plot_emissions(states, gaussian_ems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emission Distributions can be any valid distribution!\n",
    "\n",
    "Nobody said we have to use Gaussian distributions for emissions; we can, in fact, have a ton of fun and start simulating data using other distributions!\n",
    "\n",
    "Let's try Poisson emissions. Here, then, the poisson rate $\\lambda$ is given one per state. In our example below:\n",
    "\n",
    "1. State 1 gives us data $y_1 \\sim Pois(\\lambda=1)$\n",
    "1. State 2 gives us data $y_2 \\sim Pois(\\lambda=10)$\n",
    "1. State 3 gives us data $y_3 \\sim Pois(\\lambda=50)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "def poisson_emissions(states: List[int], lam: List[float]) -> List[int]:\n",
    "    emissions = []\n",
    "    for state in states:\n",
    "        rate = lam[state]\n",
    "        e = poisson.rvs(rate)\n",
    "        emissions.append(e)\n",
    "    return emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, let's observe the emissions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_ems = poisson_emissions(states, lam=[1, 10, 50])\n",
    "plot_emissions(states, poisson_ems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope the point is made: Take your favourite distribution and use it as the emission distribution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive Emissions\n",
    "\n",
    "Autoregressive emissions make things even more interesting and flexible! The \"autoregressive\" component tells us that the emission value does not only depend on the current state, but also on previous state(s).\n",
    "\n",
    "How, though, can we enforce this dependency structure? Well, as implied by the term \"structure\", it means we have some set of equations that relate the parameters of the emission distribution to the value of the previous emission.\n",
    "\n",
    "### Heteroskedastic Autoregressive Emissions\n",
    "\n",
    "Here's a \"simple complex\" example, where the location $\\mu_t$ of the emission distribution at time $t$ depends on $y_{t-1}$, and only the scale $\\sigma$ depends only on the state.\n",
    "\n",
    "$$y_t \\sim N(\\mu=k y_{t-1}, \\sigma=\\sigma_{s_t})$$\n",
    "\n",
    "Here, $k$ is an autoregressive coefficient that describes the _strength_ of dependence on the previous state. We might also assume that the initial location $\\mu=0$. Because the scale $\\sigma$ varies with state, the emissions are called **heteroskedastic**, which means \"of non-constant variance\". In the example below:\n",
    "\n",
    "1. State 1 gives us $\\sigma=0.5$ (kind of small variance).\n",
    "1. State 2 gives us $\\sigma=0.1$ (smaller variance).\n",
    "1. State 3 gives us $\\sigma=0.01$ (very small varaince).\n",
    "\n",
    "In Python code, we would model it this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar_gaussian_heteroskedastic_emissions(states: List[int], k: float, sigmas: List[float]) -> List[float]:\n",
    "    emissions = []\n",
    "    prev_loc = 0\n",
    "    for i, state in enumerate(states):\n",
    "        e = norm.rvs(loc=k * prev_loc, scale=sigmas[state])\n",
    "        emissions.append(e)\n",
    "        prev_loc = e\n",
    "    return emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_het_ems = ar_gaussian_heteroskedastic_emissions(states, k=1, sigmas=[0.5, 0.1, 0.01])\n",
    "plot_emissions(states, ar_het_ems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrast that against vanilla Gaussian emissions that are non-autoregressive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_emissions(states, gaussian_ems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind, here, that regardless of what the emissions are, it is the **variance** around the _heteroskedastic autoregressive emissions_ that gives us information about the state, _not_ the location_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the autoregressive coefficient $k$ affect the Markov chain emissions?\n",
    "\n",
    "As should be visible, the **structure** of autoregressiveness can really change how things look! What happens as $k$ changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_het_ems = ar_gaussian_heteroskedastic_emissions(states, k=1, sigmas=[0.5, 0.1, 0.01])\n",
    "plot_emissions(states, ar_het_ems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_het_ems = ar_gaussian_heteroskedastic_emissions(states, k=0, sigmas=[0.5, 0.1, 0.01])\n",
    "plot_emissions(states, ar_het_ems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting stuff! As $k \\rightarrow 0$, we approach a **heteroskedastic Gaussian random walk** centered exactly on zero (which is exactly what the mean of the Gaussian emissions would place it), where only the variance of the observations, rather than the location of the observations, give us information about the state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homoskedastic Autoregressive Emissions\n",
    "\n",
    "What if we wanted instead the variance to remain the same, but desired instead that the emission location $\\mu$ gives us information about the state while still being autoregressive? Well, we can bake that into the equation structure!\n",
    "\n",
    "$$y_t \\sim N(\\mu=k y_{t-1} + \\mu_t, \\sigma=1)$$\n",
    "\n",
    "In Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar_gaussian_homoskedastic_emissions(states: List[int], k: float, mus: List[float]) -> List[float]:\n",
    "    emissions = []\n",
    "    prev_loc = 0\n",
    "    for i, state in enumerate(states):\n",
    "        e = norm.rvs(loc=k * prev_loc + mus[state], scale=1)\n",
    "        emissions.append(e)\n",
    "        prev_loc = e\n",
    "    return emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_hom_ems = ar_gaussian_homoskedastic_emissions(states, k=1, mus=[-10, 0, 10])\n",
    "plot_emissions(states, ar_hom_ems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance is too small relative to the scale of the data, so it looks like smooth lines.\n",
    "\n",
    "If we change $k$, however, we get interesting effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_hom_ems = ar_gaussian_homoskedastic_emissions(states, k=0.8, mus=[-10, 0, 10])\n",
    "plot_emissions(states, ar_hom_ems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we get \"smoother\" transitions into each state. It's less jumpy. This is extremely useful for modelling motion activity, for example, where people move into and out of states without having jumpy-switching. (We don't go from sitting to standing to walking by jumping frames, we ease into each.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Autoregressive Homoskedastic Emissions\n",
    "\n",
    "With non-autoregressive homoskedastic emissions, the mean gives us information, but the scale doesn't, and at the same time, the mean depends only on the state, and not on the previous state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_homoskedastic_emissions(states: List[int], mus: List[float]) -> List[float]:\n",
    "    emissions = []\n",
    "    \n",
    "    prev_loc = 0\n",
    "    for i, state in enumerate(states):\n",
    "        e = norm.rvs(loc=mus[state], scale=1)\n",
    "        emissions.append(e)\n",
    "        prev_loc = e\n",
    "    return emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hom_ems = gaussian_homoskedastic_emissions(states, mus=[-10, 0, 10])\n",
    "plot_emissions(states, hom_ems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might intuit from looking at the equations, this is nothing more than a special case of the Heteroskedastic Gaussian Emissions example shown much earlier above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of MMs all the way to AR-HMMs\n",
    "\n",
    "There's the plain old **Markov Model**, in which we might generate a sequence of states $S$, which are generated from some initial distribution and transition matrix.\n",
    "\n",
    "$$ p_S = \n",
    "\\begin{pmatrix} p_1 & p_2 & p_3\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$ p_T = \n",
    "\\begin{pmatrix}\n",
    "    p_{11} & p_{12} & p_{13}\\\\\n",
    "    p_{21} & p_{22} & p_{23}\\\\\n",
    "    p_{31} & p_{32} & p_{33}\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$S = \\{s_t, s_{t+1}, ... s_{t+n}\\}$$\n",
    "\n",
    "Then there's the **\"Hidden\" Markov Model**, in which we don't observe the states but rather the _emissions_ generated from the states (according to some assumed distribution). Now, there's not only the initial distribution and transition matrix to worry about, but also the distribution of the emissions conditioned on the state. The general case is when we have some artbitrary distribution (i.e. the Gaussian or the Poisson or the Chi-Squared - whichever fits the likelihood of your data best).\n",
    "\n",
    "$$y_t|s_t \\sim Dist(\\theta_{t})$$\n",
    "\n",
    "Where $\\theta_t$ refers to the parameters for the generic distribution $Dist$ that are indexed by the state $s_t$. Your distributions probably generally come from the same family (e.g. \"Gaussians\"), or you can go super complicated and generate them from different distributions.\n",
    "\n",
    "In special cases, the parameters of the _emission distribution_ can be held constant (i.e. simple random walks), or they can depend on the state (i.e. basic HMMs). If you make the variance of the likelihood distribution vary based on state, you get **heteroskedastic** HMMs; conversely, if you keep the variance constant, then you have **homoskedastic** HMMs.\n",
    "\n",
    "Then, there's the **\"Autoregressive\" Hidden Markov Models**, in which the emissions generated from the states have a dependence on the previous states. Here, we have the ultimate amount of flexibility to model our processes.\n",
    "\n",
    "$$y_t|s_t \\sim Dist(f(y_{t-1}, \\theta_t))$$\n",
    "\n",
    "To keep things simple in this essay, we've only considered the case of lag of 1 (which is where the $t-1$ comes from). However, arbitrary numbers of time lags are possible too!\n",
    "\n",
    "And, as usual, you can make them homoskedastic or heteroskedastic by simply controlling the variance parameter of the $Dist$ distribution.\n",
    "\n",
    "Bonus point: your inputs don't necessarily have to be single dimensional; they can be multidimensional too! As long as you write the $f(y_{t-1}, \\theta_t)$ in a fashion that handles $y$ that are multidimensional, you're golden! Moreover, you can also write the function $f$ to be any function you like; it doesn't have to be a linear function (like we did); it can instead be a neural network if you so choose to do so, thus giving you a natural progression from Markov models to Recurrent Neural Networks. That, however, is out of scope for this essay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference on Markov Models\n",
    "\n",
    "Now that we've gone through the \"data generating process\" for Markov sequences with emissions, we can re-examine the entire class of models in a Bayesian light.\n",
    "\n",
    "If you've been observing the models that we've been \"forward-simulating\" all this while to generate data, you'll notice that there are a few key parameters that seemed like, \"well, if we changed them, then the data would change, right?\" If that's what you've been thinking, then bingo! You're on the right track.\n",
    "\n",
    "Moreover, you'll notice that I've couched everything in the language of probability distributions. The transition probabilities $P(s_t | s_{t-1})$ are given by a Multinomial distribution. The emission probabilities are given by an arbitrary continuous (or discrete) distribution, depending on what the likelihood of the data are. Given that we're working with probability distributions and data, you probably have been thinking about it already: we need a way to calculate the log-likelihoods of the data that we observe!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain Log-Likelihood Calculation\n",
    "\n",
    "Let's examine how we would calculate the log likelihood of **state data** given the parameters. This will lead us to the Markov chain log-likelihood.\n",
    "\n",
    "Since $P(s_t|s_{t-1})$ is a **multinomial distribution**, then if we are given the log-likelihood of $\\{s_1, s_2, s_3, ..., s_n\\}$, we can calculate the log-likelihood over $\\{s_2,... s_n\\}$, which is given by the sum of the log probabilities. This follows from the factorization of a Markov chain, which is out of scope for this essay, so if this trips you up, don't worry - take a hiatus from the essay and draw it out. Otherwise, take my word for it for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_logp(states, p_transition):\n",
    "    logp = 0\n",
    "    \n",
    "    # states are 0, 1, 2, but we model them as [1, 0, 0], [0, 1, 0], [0, 0, 1]\n",
    "    states_oh = np.eye(len(p_transition))\n",
    "    for curr_state, next_state in zip(states[:-1], states[1:]):\n",
    "        p_tr = p_transition[curr_state]        \n",
    "        logp += multinomial(n=1, p=p_tr).logpmf(states_oh[next_state])\n",
    "    return logp\n",
    "\n",
    "state_logp(states, p_transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also write a vectorized version of `state_logp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_logp_vect(states, p_transition):\n",
    "    states_oh = np.eye(len(p_transition))\n",
    "    p_tr = p_transition[states[:-1]]\n",
    "    obs = states_oh[states[1:]]\n",
    "    return np.sum(multinomial(n=1, p=p_tr).logpmf(obs))\n",
    "\n",
    "state_logp_vect(states, p_transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there is a problem here: we also need the log likelihood of the first state.\n",
    "\n",
    "If we don't know what the initial distribution is supposed to be, one possible assumption we can make is that the Markov sequence began by drawing from the equilibrium distribution. Here is where equilibrium distribution calculation from before comes in handy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_logp(states, p_transition):\n",
    "    initial_state = states[0]\n",
    "    states_oh = np.eye(len(p_transition))\n",
    "    eq_p = equilibrium_distribution(p_transition)\n",
    "    return (\n",
    "        multinomial(n=1, p=p_transition[initial_state])\n",
    "        .logpmf(states_oh[initial_state].squeeze())\n",
    "    )\n",
    "\n",
    "initial_logp(states, p_transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken together, we get the following Markov chain log-likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_state_logp(states, p_transition):\n",
    "    return (\n",
    "        state_logp_vect(states, p_transition)\n",
    "        + initial_logp(states, p_transition)\n",
    "    )\n",
    "\n",
    "markov_state_logp(states, p_transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain with Gaussian Emissions Log-Likelihood Calculation\n",
    "\n",
    "Now that we know how to calculate the log-likelihood for the Markov chain sequence of states,\n",
    "we can now move on to the log-likelihood calculation for the emissions.\n",
    "\n",
    "Let's first assume that we have emissions that are non-autoregressive, and have a Gaussian likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_logp(states, mus, sigmas, emissions):\n",
    "    logp = 0\n",
    "    for (emission, state) in zip(emissions, states):\n",
    "        logp += norm(mus[state], sigmas[state]).logpdf(emission)\n",
    "    return logp\n",
    "\n",
    "gaussian_logp(states, mus=[1, 0, -1], sigmas=[0.2, 0.5, 0.1], emissions=gaussian_ems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll also make a vectorized version of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_logp_vect(states, mus, sigmas, emissions):\n",
    "    mu = mus[states]\n",
    "    sigma = sigmas[states]\n",
    "    return np.sum(norm(mu, sigma).logpdf(emissions))\n",
    "\n",
    "gaussian_logp_vect(states, mus=np.array([1, 0, -1]), sigmas=np.array([0.2, 0.5, 0.1]), emissions=gaussian_ems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint log likelihood of the emissions and states are then given by their summation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_emission_hmm_logp(states, p_transition, mus, sigmas, emissions):\n",
    "    return markov_state_logp(states, p_transition) + gaussian_logp_vect(states, mus, sigmas, emissions)\n",
    "\n",
    "gaussian_emission_hmm_logp(states, p_transition, mus=np.array([1, 0, -1]), sigmas=np.array([0.2, 0.5, 0.1]), emissions=gaussian_ems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're in a Binder or local Jupyter session, go ahead and tweak the values of `mus` and `sigmas`, and verify for yourself that with the current values passed in, they are the \"maximum likelihood\" values. After all, our Gaussian emission data were generated according to this exact set of parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain with Autoregressive Gaussian Emissions Log-Likelihood Calculation\n",
    "\n",
    "I hope the pattern is starting to be clear here: since we have Gaussian emissions, we only have to calculate the parameters of the Gaussian to know what the logpdf would be.\n",
    "\n",
    "As an example, I will be using the Gaussian with:\n",
    "\n",
    "- State-varying scale\n",
    "- Mean that is dependent on the previously emitted value\n",
    "\n",
    "This is the AR-HMM with data generated from the `ar_gaussian_heteroskedastic_emissions` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar_gaussian_heteroskedastic_emissions_logp(states, k, sigmas, emissions):\n",
    "    logp = 0\n",
    "    initial_state = states[0]\n",
    "    initial_emission_logp = norm(0, sigmas[initial_state]).logpdf(emissions[0])\n",
    "    for previous_emission, current_emission, state in zip(emissions[:-1], emissions[1:], states[1:]):\n",
    "        loc = k * previous_emission\n",
    "        scale = sigmas[state]\n",
    "        logp += norm(loc, scale).logpdf(current_emission)\n",
    "    return logp\n",
    "\n",
    "ar_gaussian_heteroskedastic_emissions_logp(states, k=1.0, sigmas=[0.5, 0.1, 0.01], emissions=ar_het_ems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can write the full log likelihood of the entire AR-HMM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar_gausian_heteroskedastic_hmm_logp(states, p_transition, k, sigmas, emissions):\n",
    "    return (\n",
    "        markov_state_logp(states, p_transition)\n",
    "        + ar_gaussian_heteroskedastic_emissions_logp(states, k, sigmas, emissions)\n",
    "    )\n",
    "\n",
    "\n",
    "ar_gausian_heteroskedastic_hmm_logp(states, p_transition, k=1.0, sigmas=[0.5, 0.1, 0.01], emissions=ar_het_ems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those of you who are familiar with Bayesian inference, as soon as we have a log likelihood that we can calculate, once we tack on priors, using the simple Bayes' rule equation, we can obtain posterior distributions easily by chaining on an MCMC sampler.\n",
    "\n",
    "If this looks all foreign to you, then be check out my [other essay](https://ericmjl.github.io/essays-on-data-science/machine-learning/computational-bayesian-stats/) for a first look (or a refresher)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM Distributions in PyMC3\n",
    "\n",
    "While PyMC4 is in development, PyMC3 remains one of the leading probabilistic programming languages that can be used for Bayesian inference. PyMC3 doesn't have the HMM distribution defined in the library, but thanks to GitHub user **@hstrey** [posting a Jupyter notebook with HMMs defined in there](https://github.com/hstrey/Hidden-Markov-Models-pymc3/blob/master/Multi-State%20HMM.ipynb), many PyMC3 users have had a great baseline distribution to study pedagogically and use in their applications, myself included.\n",
    "\n",
    "_Side note: I used @hstrey's implementation before setting out to write this essay. Thanks!_\n",
    "\n",
    "### HMM States Distribution\n",
    "\n",
    "Let's first look at the HMM States distribution, which will give us a way to calculate the log probability of the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "import theano.tensor.slinalg as sla  # theano-wrapped scipy linear algebra\n",
    "import theano\n",
    "\n",
    "theano.config.gcc.cxxflags = \"-Wno-c++11-narrowing\"\n",
    "\n",
    "class HMMStates(pm.Categorical):\n",
    "    def __init__(self, p_transition, p_equilibrium, n_states, *args, **kwargs):\n",
    "        super(pm.Categorical, self).__init__(*args, **kwargs)\n",
    "        self.p_transition = p_transition\n",
    "        self.p_equilibrium = p_equilibrium\n",
    "        # This is needed\n",
    "        self.k = n_states\n",
    "        # This is only needed because discrete distributions must define a mode.\n",
    "        self.mode = tt.cast(0,dtype='int64')\n",
    "\n",
    "    def logp(self, x):\n",
    "        p_eq = self.p_equilibrium\n",
    "        # Broadcast out the transition probabilities.\n",
    "        p_tr = self.p_transition[x[:-1]]\n",
    "        \n",
    "        # the logp of the initial state evaluated against the equilibrium probabilities\n",
    "        initial_state_logp = pm.Categorical.dist(p_eq).logp(x[0])\n",
    "\n",
    "        # the logp of the rest of the states.\n",
    "        x_i = x[1:]\n",
    "        ou_like = pm.Categorical.dist(p_tr).logp(x_i)\n",
    "        transition_logp = tt.sum(ou_like)\n",
    "        return initial_state_logp + transition_logp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, the categorical distribution is used for convenience - it can handle integers, while multinomial requires the one-hot transformation.\n",
    "\n",
    "Now, we stated earlier on that the transition matrix can be treated as a parameter to tweak, or else a random variable for which we want to infer its parameters. This means there is a natural fit for placing priors on them! Dirichlet distributions are great priors for probability vectors, as they are the generalization of Beta distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_equilibrium(n_states, p_transition):\n",
    "    A = tt.dmatrix('A')\n",
    "    A = tt.eye(n_states) - p_transition + tt.ones(shape=(n_states, n_states))\n",
    "    p_equilibrium = pm.Deterministic(\"p_equilibrium\", sla.solve(A.T, tt.ones(shape=(n_states))))\n",
    "    return p_equilibrium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_states = 3\n",
    "with pm.Model() as model:\n",
    "    p_transition = pm.Dirichlet(\"p_transition\", a=tt.ones((n_states, n_states)), shape=(n_states, n_states))\n",
    "    \n",
    "    # Solve for the equilibrium state\n",
    "    p_equilibrium = solve_equilibrium(n_states, p_transition)\n",
    "\n",
    "    obs_states = HMMStates(\n",
    "        \"states\", \n",
    "        p_transition=p_transition, \n",
    "        p_equilibrium=p_equilibrium, \n",
    "        n_states=n_states,\n",
    "        observed=np.array(states).astype(\"float\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = pm.sample(2000, cores=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "az.plot_forest(trace, var_names=[\"p_transition\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we were able to recover the original transitions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMM with Gaussian Emissions\n",
    "\n",
    "Let's try out now an HMM model with Gaussian emissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMMGaussianEmissions(pm.Continuous):\n",
    "    def __init__(self, states, mu, sigma, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.states = states\n",
    "        # self.rate = rate\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def logp(self, x):\n",
    "        \"\"\"\n",
    "        x: observations\n",
    "        \"\"\"\n",
    "        states = self.states\n",
    "        # rate = self.rate[states]  # broadcast the rate across the states.\n",
    "        mu = self.mu[states]\n",
    "        sigma = self.sigma[states]\n",
    "        return tt.sum(pm.Normal.dist(mu=mu, sigma=sigma).logp(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 3\n",
    "with pm.Model() as model:\n",
    "    # Priors for transition matrix\n",
    "    p_transition = pm.Dirichlet(\"p_transition\", a=tt.ones((n_states, n_states)), shape=(n_states, n_states))\n",
    "    \n",
    "    # Solve for the equilibrium state\n",
    "    p_equilibrium = solve_equilibrium(n_states, p_transition)\n",
    "\n",
    "    # HMM state\n",
    "    hmm_states = HMMStates(\n",
    "        \"hmm_states\", \n",
    "        p_transition=p_transition, \n",
    "        p_equilibrium=p_equilibrium, \n",
    "        n_states=n_states, \n",
    "        shape=(len(poisson_ems),)\n",
    "    )\n",
    "    \n",
    "    # Prior for mu and sigma\n",
    "    mu = pm.Normal(\"mu\", mu=0, sigma=1, shape=(n_states,))\n",
    "    sigma = pm.Exponential(\"sigma\", lam=2, shape=(n_states,))\n",
    "    \n",
    "    # Observed emission likelihood\n",
    "    obs = HMMGaussianEmissions(\n",
    "        \"emission\", \n",
    "        states=hmm_states, \n",
    "        mu=mu, \n",
    "        sigma=sigma, \n",
    "        observed=gaussian_ems\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = pm.sample(2000, cores=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, var_names=[\"rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, var_names=[\"sigma\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace, var_names=[\"sigma\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to recover the right sigmas, but they are split amongst the chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian-analysis-recipes",
   "language": "python",
   "name": "bayesian-analysis-recipes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
