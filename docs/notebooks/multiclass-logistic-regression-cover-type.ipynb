{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import patsy\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**Multi-class Bayesian Logistic Regression**\n",
    "\n",
    "The task at hand is to predict forest cover type from cartographic variables. See https://archive.ics.uci.edu/ml/datasets/covertype for more details. \n",
    "\n",
    "The purpose of this notebook is more to give an overview on how to do **multi-class logistic regression** using PyMC3.\n",
    "\n",
    "# Data Pre-Processing\n",
    "\n",
    "Firstly, let's read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/covtype_preprocess.csv\", index_col=0)\n",
    "df.shape\n",
    "\n",
    "df[\"Cover_Type\"] = df[\"Cover_Type\"].apply(lambda x: str(x))\n",
    "\n",
    "output_col = \"Cover_Type\"\n",
    "input_cols = [c for c in df.columns if c != output_col]\n",
    "input_formula = \"\".join(c + \" + \" for c in input_cols)\n",
    "input_formula = input_formula + \"-1\"\n",
    "\n",
    "import patsy\n",
    "from sklearn.preprocessing import scale, normalize\n",
    "\n",
    "X = patsy.dmatrix(formula_like=input_formula, data=df, return_type=\"dataframe\")\n",
    "# X = normalize(X)\n",
    "\n",
    "Y = patsy.dmatrix(formula_like=\"Cover_Type -1\", data=df, return_type=\"dataframe\")\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variables\n",
    "\n",
    "Firstly, let's get the target variables out as a multi-class table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_col = \"Cover_Type\"\n",
    "input_cols = [c for c in df.columns if c != output_col]\n",
    "input_formula = \"\".join(c + \" + \" for c in input_cols)\n",
    "input_formula = input_formula + \"-1\"\n",
    "\n",
    "import patsy\n",
    "from sklearn.preprocessing import scale, normalize\n",
    "\n",
    "X = patsy.dmatrix(formula_like=input_formula, data=df, return_type=\"dataframe\")\n",
    "# X = normalize(X)\n",
    "\n",
    "Y = patsy.dmatrix(formula_like=\"Cover_Type -1\", data=df, return_type=\"dataframe\")\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalance\n",
    "\n",
    "Is there class imbalance in the dataset? Let's check this to see if we need to do some downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, there is class imbalance. Target 4 is about 100X less than targets 1 and 2, and about 10X less than targets 6 and 7. Need to downsample to that size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling\n",
    "\n",
    "We will downsample the data to just 2747 datapoints, and normalize the data using `scikit-learn`'s `normalize` function from the `sklearn.preprocessing` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_targets = []\n",
    "\n",
    "for i in range(1, 7 + 1):\n",
    "    # print(f'target[{i}]')\n",
    "    target = Y[Y[f\"Cover_Type[{i}]\"] == 1]\n",
    "    # print(len(target))\n",
    "    downsampled_targets.append(target.sample(2747))\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "X = pm.floatX(mms.fit_transform(df[input_cols]))\n",
    "\n",
    "Y_downsamp = pd.concat(downsampled_targets)\n",
    "X_downsamp = X[Y_downsamp.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sanity Checks\n",
    "\n",
    "Let's now check that the downsampled classes are indeed of the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_downsamp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolor(Y_downsamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_downsamp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_downsamp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape[1], Y.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's visualize the distribution of data.\n",
    "\n",
    "### Missing Data\n",
    "\n",
    "Firstly, checking for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "\n",
    "msno.matrix(pd.DataFrame(X_downsamp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min/Max Distribution\n",
    "\n",
    "Next up, checking for distribution of min/max values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.pcolor(X_downsamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am satisfied that the data have been normalized correctly, and are in the correct shape. The caveats of correlations between columns still remain, but I won't deal with them for now, as I just want to get multi-class logistic regression going first.\n",
    "\n",
    "# Model Construction\n",
    "\n",
    "I have chosen to do Bayesian logistic regression. In the original work, neural networks (NNs) were used. Because of the increased modelling capacity of NNs, I expect that they will perform better than Bayesian LR.\n",
    "\n",
    "Nonetheless, as this is an exercise in implementing simple Bayesian models for others to use as a recipe for their own analyses, I will focus on model construction and critique, and not on model comparison. Thus, no cross-validation.\n",
    "\n",
    "## Logit Function\n",
    "\n",
    "With that said, let's move onto the model. Firstly, we define the logit function:\n",
    "\n",
    "$$ logit(X) = \\frac{1}{1 + e^{-X}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(X):\n",
    "    return 1 / (1 + np.exp(-X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick test that the logit function will indeed work with an array of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit(np.array([1, 2, 1, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specification \n",
    "\n",
    "Now, we implement the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian multi-class logistic regression to predict the target classes.\n",
    "import theano.tensor as tt\n",
    "\n",
    "with pm.Model() as model:\n",
    "    weights_testval = np.random.randn(X_downsamp.shape[1], Y_downsamp.shape[1])\n",
    "    weights = pm.Normal(\n",
    "        \"weights\", 0.0, 100.0, shape=(X_downsamp.shape[1], Y_downsamp.shape[1])\n",
    "    )\n",
    "\n",
    "    intercept = pm.Normal(\"intercept\", 0.0, 100.0, shape=Y_downsamp.shape[1])\n",
    "\n",
    "    exponent = tt.dot(X_downsamp, weights) + intercept\n",
    "    p = logit(exponent)\n",
    "\n",
    "    like = pm.Multinomial(\"likelihood\", n=1, p=p, observed=np.asarray(Y_downsamp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some quick checks, with thanks to @junpenglao for providing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.logp(model.test_point))\n",
    "for RV in model.basic_RVs:\n",
    "    print(RV.name, RV.logp(model.test_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = pm.sample(50000, start=pm.find_MAP(), step=pm.Metropolis())\n",
    "    # trace = pm.sample(2000, step=pm.Metropolis())  # NaN occurs in optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traces\n",
    "\n",
    "Visualize the traces to check for convergence in sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "\n",
    "Sampling is pretty good. No trends in the intercepts or weights. In fact, putting 7 intercept terms (one for each class) caused shrinkage (not in the Bayesian hierarchical sense) of the weights to be closer to zeros, which I think is a good sign.\n",
    "\n",
    "# Model Evaluation\n",
    "\n",
    "We will use posterior predictive checks to sample out new data from the posterior distributions of weights and intercepts. \n",
    "\n",
    "## Sample PPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    ppc_samps = pm.sample_ppc(trace, samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc_samps[\"likelihood\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of multi-class classification, let's label the class that has the highest probability to be the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = ppc_samps[\"likelihood\"].mean(axis=0)\n",
    "ppc_preds = probs == np.max(probs, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolor(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolor(ppc_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scikitplot.plotters import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_downsamp, ppc_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian",
   "language": "python",
   "name": "bayesian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "593px",
    "left": "0px",
    "right": "1068px",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
