{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import pymc4 as pm4\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import janitor as jn\n",
    "from utils import ecdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Type\n",
    "\n",
    "The Bayesian estimation model is widely applicable across a number of scenarios. The classical scenario is when we have an experimental design where there is a control vs. a treatment, and we want to know what the difference is between the two. Here, \"estimation\" is used to estimate the \"true\" value for the control and the \"true\" value for the treatment, and the \"Bayesian\" part refers to the computation of the uncertainty surrounding the parameter. \n",
    "\n",
    "Bayesian estimation's advantages over the classical t-test was first described by John Kruschke (2013). \n",
    "\n",
    "In this notebook, I provide a concise implementation suitable for two-sample and multi-sample inference, with data that don't necessarily fit Gaussian assumptions.\n",
    "\n",
    "## Data structure\n",
    "\n",
    "To use it with this model, the data should be structured as such:\n",
    "\n",
    "- Each row is one measurement.\n",
    "- The columns should indicate, at the minimum:\n",
    "    - What treatment group the sample belonged to.\n",
    "    - The measured value.\n",
    "\n",
    "## Extensions to the model\n",
    "\n",
    "As of now, the model only samples posterior distributions of measured values. The model, then, may be extended to compute differences in means (sample vs. control) or effect sizes, complete with uncertainty around it. Use `pm.Deterministic(...)` to ensure that those statistics' posterior distributions, i.e. uncertainty, are also computed.\n",
    "\n",
    "## Reporting summarized findings\n",
    "\n",
    "Here are examples of how to summarize the findings.\n",
    "\n",
    "> Treatment group A was greater than control by x units (95% HPD: [`lower`, `upper`]). \n",
    "\n",
    "> Treatment group A was higher than control (effect size 95% HPD: [`lower`, `upper`]). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "df = (\n",
    "    pd.read_csv(\"../datasets/biofilm.csv\")\n",
    "    .label_encode(columns=[\"isolate\"])  # encode isolate as labels.\n",
    "    .transform_column(\"normalized_measurement\", np.log, \"log_normalized_measurement\")\n",
    ")\n",
    "\n",
    "# Convert continuous columns to floatX for GPU compatibility.\n",
    "continuous_cols = [\"OD600\", \"ST\", \"replicate\", \"measurement\", \"normalized_measurement\"]\n",
    "for c in continuous_cols:\n",
    "    df[c] = pm.floatX(df[c])\n",
    "\n",
    "# Display a subset of the data.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=\"isolate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Specification\n",
    "\n",
    "We know that the `OD600` and `measurements` columns are all positive-valued, and so the `normalized_measurement` column will also be positive-valued.\n",
    "There are two ways to handle this situation:\n",
    "\n",
    "1. We can either choose to directly model the likelihood using a bounded, positive-support-only distribution, or\n",
    "2. We can model the log-transformation of the `normalized_measurement` column, using an unbounded, infinite-support distribution (e.g. the T-distribution family of distributions, which includes the Gaussian and the Cauchy in there).\n",
    "\n",
    "The former is ever slightly more convenient to reason about, but the latter lets us use Gaussians, which have some nice properties when sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "num_isolates = len(set(df[\"isolate_enc\"]))\n",
    "@pm4.model\n",
    "def bacteria_model():\n",
    "    \n",
    "    mu_mean = yield pm4.Normal(\"mu_mean\", loc=0, scale=1)\n",
    "    mu = yield pm4.Normal(\"mu\", loc=mu_mean, scale=1, batch_stack=num_isolates)\n",
    "    mu_bounded = yield pm4.Deterministic(\"mu_bounded\", tf.exp(mu))\n",
    "    \n",
    "    # Because we use TFP, tf.gather now replaces the old numpy syntax.\n",
    "    # the following line is equivalent to:\n",
    "    #    mu_all = mu[df[\"isolate_enc\"]]\n",
    "    mu_all = tf.gather(mu, df[\"isolate_enc\"])\n",
    "    \n",
    "    sigma = yield pm4.HalfCauchy(\"sd\", scale=1, batch_stack=num_isolates)\n",
    "    sigma_all = tf.gather(sigma, df[\"isolate_enc\"])\n",
    "    \n",
    "    nu = yield pm4.Exponential(\"nu\", rate=1/30.)\n",
    "    \n",
    "    like = yield pm4.StudentT(\"like\", loc=mu_all, scale=sigma_all, df=nu, observed=df[\"log_normalized_measurement\"])\n",
    "    \n",
    "    # Take the difference against the ATCC strain, which is the control.\n",
    "    difference = yield pm4.Deterministic(\"difference\", mu_bounded[:-1] - mu_bounded[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = pm4.sample(bacteria_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "az.plot_trace(trace, var_names=[\"bacteria_model/mu\"], compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the traces, yes, everything looks more or less like a hairy caterpillar. This means that sampling went well, and has converged, thus we have a good MCMC estimator of the posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need a mapping of isolate to its encoding - will come in handy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = dict(zip(df[\"isolate_enc\"], df[\"isolate\"]))\n",
    "yticklabels = list(reversed([mapping[i] for i in range(len(mapping))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = az.plot_forest(trace, var_names=[\"bacteria_model/mu_bounded\"])\n",
    "axes[0].set_yticklabels(yticklabels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the basis of this, we would say that strain 5 was the most different from the other strains.\n",
    "\n",
    "Let's now look at the differences directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = az.plot_forest(trace, var_names=[\"bacteria_model/difference\"])\n",
    "axes[0].axvline(0, color=\"black\")\n",
    "axes[0].set_yticklabels(yticklabels[1:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were in a binary decision-making mode, we would say that isolates 1, 14, 15 and 5 were the most \"significantly\" different from the ATCC strain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Check\n",
    "\n",
    "Let's see if we can generate PPC samples that look similar. Admittedly, there's a bit of an art to checking here - there's only 6 measurements per strain, so it's not like we have a lot of data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = pm4.sample_posterior_predictive(bacteria_model(), trace=trace, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.posterior_predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want indices for each of the samples.\n",
    "indices = dict()\n",
    "for enc, iso in mapping.items():\n",
    "    idxs = list(df[df[\"isolate_enc\"] == enc].index)\n",
    "    indices[iso] = idxs\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.posterior_predictive[\"bacteria_model/like\"].loc[:, :, indices[\"1\"]].mean(axis=2)#.data.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make PPC plot for one of the groups.\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "gs = GridSpec(nrows=4, ncols=4)\n",
    "axes = dict()\n",
    "\n",
    "\n",
    "for i, (strain, idxs) in enumerate(indices.items()):\n",
    "    if i > 0:\n",
    "        ax = fig.add_subplot(gs[i], sharex=axes[0])\n",
    "    else:\n",
    "        ax = fig.add_subplot(gs[i])\n",
    "    x, y = ecdf(df.iloc[idxs][\"log_normalized_measurement\"])\n",
    "    ax.plot(x, y, label=\"data\")\n",
    "    x, y = ecdf(\n",
    "        trace\n",
    "        .posterior_predictive[\"bacteria_model/like\"]\n",
    "        .loc[:, :, idxs]\n",
    "        .mean(axis=(2))\n",
    "        .data\n",
    "        .flatten()\n",
    "    )\n",
    "    ax.plot(x, y, label=\"ppc\")\n",
    "    ax.set_title(f\"Strain {strain}\")\n",
    "    axes[i] = ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PPC draws clearly have longer tails than do the originals.\n",
    "I chalk this down to having small number of samples. \n",
    "The central tendency is definitely modelled well,\n",
    "and I don't see wild deviations between the sampled posterior and the measured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian-analysis-recipes",
   "language": "python",
   "name": "bayesian-analysis-recipes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "102px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
