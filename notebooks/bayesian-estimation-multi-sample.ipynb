{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import pymc4 as pm4\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import janitor as jn\n",
    "from utils import ecdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Type\n",
    "\n",
    "The Bayesian estimation model is widely applicable across a number of scenarios. The classical scenario is when we have an experimental design where there is a control vs. a treatment, and we want to know what the difference is between the two. Here, \"estimation\" is used to estimate the \"true\" value for the control and the \"true\" value for the treatment, and the \"Bayesian\" part refers to the computation of the uncertainty surrounding the parameter. \n",
    "\n",
    "Bayesian estimation's advantages over the classical t-test was first described by John Kruschke (2013). \n",
    "\n",
    "In this notebook, I provide a concise implementation suitable for two-sample and multi-sample inference.\n",
    "\n",
    "## Data structure\n",
    "\n",
    "To use it with this model, the data should be structured as such:\n",
    "\n",
    "- Each row is one measurement.\n",
    "- The columns should indicate, at the minimum:\n",
    "    - What treatment group the sample belonged to.\n",
    "    - The measured value.\n",
    "\n",
    "## Extensions to the model\n",
    "\n",
    "As of now, the model only samples posterior distributions of measured values. The model, then, may be extended to compute differences in means (sample vs. control) or effect sizes, complete with uncertainty around it. Use `pm.Deterministic(...)` to ensure that those statistics' posterior distributions, i.e. uncertainty, are also computed.\n",
    "\n",
    "## Reporting summarized findings\n",
    "\n",
    "Here are examples of how to summarize the findings.\n",
    "\n",
    "> Treatment group A was greater than control by x units (95% HPD: [`lower`, `upper`]). \n",
    "\n",
    "> Treatment group A was higher than control (effect size 95% HPD: [`lower`, `upper`]). \n",
    "\n",
    "## Other notes\n",
    "\n",
    "Here, we make a few modelling choices.\n",
    "\n",
    "1. We care only about the `normalized_measurement` column, and so we choose the t-distribution to model it, as we don't have a good \"mechanistic\" model that incorporates measurement error of OD600 and 'measurement'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "df = (\n",
    "    pd.read_csv(\"../datasets/biofilm.csv\")\n",
    "    .label_encode(columns=[\"isolate\"])  # encode isolate as labels.\n",
    "    .transform_column(\"normalized_measurement\", np.log, \"log_normalized_measurement\")\n",
    ")\n",
    "\n",
    "# Convert continuous columns to floatX for GPU compatibility.\n",
    "continuous_cols = [\"OD600\", \"ST\", \"replicate\", \"measurement\", \"normalized_measurement\"]\n",
    "for c in continuous_cols:\n",
    "    df[c] = pm.floatX(df[c])\n",
    "\n",
    "# Display a subset of the data.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Specification\n",
    "\n",
    "We know that the `OD600` and `measurements` columns are all positive-valued, and so the `normalized_measurement` column will also be positive-valued.\n",
    "There are two ways to handle this situation:\n",
    "\n",
    "1. We can either choose to directly model the likelihood using a bounded, positive-support-only distribution, or\n",
    "2. We can model the log-transformation of the `normalized_measurement` column, using an unbounded, infinite-support distribution (e.g. the T-distribution family of distributions, which includes the Gaussian and the Cauchy in there).\n",
    "\n",
    "The former is ever slightly more convenient to reason about, but the latter lets us use Gaussians, which have some nice properties when sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "num_isolates = len(set(df[\"isolate_enc\"]))\n",
    "@pm4.model\n",
    "def bacteria_model():\n",
    "    \n",
    "    mu = yield pm4.Normal(\"mu\", loc=1, scale=1, batch_stack=num_isolates)\n",
    "    mu_bounded = yield pm4.Deterministic(\"mu_bounded\", tf.exp(mu))\n",
    "    \n",
    "    # Because we use TFP, tf.gather now replaces the old numpy syntax.\n",
    "    # the following line is equivalent to:\n",
    "    #    mu_all = mu[df[\"isolate_enc\"]]\n",
    "    mu_all = tf.gather(mu, df[\"isolate_enc\"])\n",
    "    \n",
    "    sigma = yield pm4.HalfCauchy(\"sd\", scale=1, batch_stack=num_isolates)\n",
    "    sigma_all = tf.gather(sigma, df[\"isolate_enc\"])\n",
    "    \n",
    "    nu = yield pm4.Exponential(\"nu\", rate=1/30.)\n",
    "    \n",
    "    like = yield pm4.StudentT(\"like\", loc=mu_all, scale=sigma_all, df=nu, observed=df[\"log_normalized_measurement\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = pm4.sample(bacteria_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "az.plot_trace(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace, var_names=[\"bacteria_model/mu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_isolates = len(set(df[\"isolate_enc\"]))\n",
    "with pm.Model() as best:\n",
    "    nu = pm.Exponential(\"degrees_of_freedom\", lam=1 / 30) + 1\n",
    "\n",
    "    log_normalized_measurement = pm.Normal(\"log_normalized_measurement\", shape=num_isolates)\n",
    "    measurement = pm.Deterministic(\"measurement\", np.exp(log_normalized_measurement))\n",
    "\n",
    "    var = pm.HalfCauchy(\"var\", beta=1, shape=num_isolates)\n",
    "\n",
    "    mu = log_normalized_measurement[df[\"isolate_enc\"].values]\n",
    "    sd = var[df[\"isolate_enc\"].values]\n",
    "\n",
    "    like = pm.StudentT(\n",
    "        \"like\", mu=mu, sd=sd, nu=nu, observed=df[\"log_normalized_measurement\"]\n",
    "    )\n",
    "\n",
    "    # Compute differences\n",
    "    diffs = pm.Deterministic(\"differences\", np.exp(log_normalized_measurement[1:]) - np.exp(log_normalized_measurement[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample from the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with best:\n",
    "    trace = pm.sample(draws=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for convergence using the traceplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "az.plot_trace(trace, var_names=[\"log_normalized_measurement\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the traces, yes, everything looks more or less like a hairy caterpillar. This means that sampling went well, and has converged, thus we have a good MCMC estimator of the posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need a mapping of isolate to its encoding - will come in handy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = dict(zip(df[\"isolate_enc\"], df[\"isolate\"]))\n",
    "mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Forest Plot to summarize how the strains differ from one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylabels = [mapping[i] for i in sorted(mapping.keys())]\n",
    "axes = az.plot_forest(trace, var_names=[\"measurement\"])\n",
    "axes[0].set_yticklabels(reversed(ylabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite clear that:\n",
    "\n",
    "- Strain 5 is very good at forming biofilms.\n",
    "- Strain 1 (control strain) is not good at forming biofilms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the difference between the strains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = az.plot_forest(trace, var_names=[\"differences\"])\n",
    "# axes[0].set_yticklabels(reversed(ylabels[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from Strain 2, none of the other strains' difference HPDs cross \"zero\".\n",
    "\n",
    "This means that they all would be considered to be \"statistically significantly different\" from strain 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the variances, just to see if there's a difference in that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace, var_names=[\"var\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that there are a number of strains for whom the variance is similar. Without better information, though, we would not be warranted to impose a similarity structure on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Check\n",
    "\n",
    "Let's see if we can generate PPC samples that look similar. Admittedly, there's a bit of an art to checking here - there's only 6 measurements per strain, so it's not like we have a lot of data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pm.sample_ppc(trace, model=best)\n",
    "samples[\"like\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want indices for each of the samples.\n",
    "indices = dict()\n",
    "for enc, iso in mapping.items():\n",
    "    idxs = list(df[df[\"isolate_enc\"] == enc].index)\n",
    "    indices[iso] = idxs\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[\"like\"][:, idxs].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make PPC plot for one of the groups.\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "gs = GridSpec(nrows=4, ncols=4)\n",
    "axes = dict()\n",
    "\n",
    "\n",
    "for i, (strain, idxs) in enumerate(indices.items()):\n",
    "    if i > 0:\n",
    "        ax = fig.add_subplot(gs[i], sharex=axes[0])\n",
    "    else:\n",
    "        ax = fig.add_subplot(gs[i])\n",
    "    x, y = ecdf(df.iloc[idxs][\"log_normalized_measurement\"])\n",
    "    ax.plot(x, y, label=\"data\")\n",
    "    x, y = ecdf(samples[\"like\"][:, idxs].mean(axis=1))\n",
    "    ax.plot(x, y, label=\"ppc\")\n",
    "    ax.set_title(f\"Strain {strain}\")\n",
    "    axes[i] = ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the PPC samples are generally okay, the StudentT distribution does give some long-tail values, including those that are negative. Given the measurements at hand, negative values would be considered \"absurd\" values. \n",
    "\n",
    "If I had more time, I might experiment with the use of a different likelihood distribution. However, this is a good enough first start model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian",
   "language": "python",
   "name": "bayesian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "102px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
