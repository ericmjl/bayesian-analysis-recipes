{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import theano.tensor as tt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to do hierarchical modelling with Binomially-distributed random variables.\n",
    "\n",
    "# Problem Setup\n",
    "\n",
    "Baseball players have many metrics measured for them. Let's say we are on a baseball team, and would like to quantify player performance, one metric being their batting average (defined by how many times a batter hit a pitched ball, divided by the number of times they were up for batting (\"at bat\")). How would you go about this task?\n",
    "\n",
    "We first need some measurements of batting data. To answer this question, we need to have data on the number of time a player has batted and the number of times the player has hit the ball while batting. Let's see an example dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/baseballdb/core/Batting.csv')\n",
    "df['AB'] = df['AB'].replace(0, np.nan)\n",
    "df = df.dropna()\n",
    "df['batting_avg'] = df['H'] / df['AB']\n",
    "df = df[df['yearID'] >= 2016]\n",
    "df = df.iloc[0:15]  # select out only the first 15 players, just for illustration purposes.\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, the columns `AB` and `H` are the most relevent.\n",
    "\n",
    "- `AB` is the number of times a player was **A**t **B**at.\n",
    "- `H` is the number of times a player **h**it the ball while batting.\n",
    "\n",
    "The performance of a player can be defined by their batting percentage - essentially the number of hits divided by the number of times at bat. (Technically, a percentage should run from 0-100, but American sportspeople are apparently not very strict with how they approach these definitions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Naive Model\n",
    "\n",
    "One model that we can write is a model that assumes that each player has a batting percentage that is independent of the other players in the dataset. \n",
    "\n",
    "A pictorial view of the model is as such:\n",
    "\n",
    "![](../images/baseball-model.jpg)\n",
    "\n",
    "Let's implement this model in PyMC3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as baseline_model:\n",
    "    thetas = pm.Beta(\"thetas\", alpha=0.5, beta=0.5, shape=(len(df)))\n",
    "    like = pm.Binomial('likelihood', n=df['AB'], p=thetas, observed=df['H'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with baseline_model:\n",
    "    baseline_trace = pm.sample(2000, njobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the posterior distribution traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traceplot = pm.traceplot(baseline_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like convergence has been achieved. From a $Beta(\\alpha=0.5, \\beta=0.5)$ prior, those players for which we have only 1 data point have very wide posterior distribution estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.forestplot(baseline_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the big problems that we may have observed above is that the posterior distribution estimates for some players look very \"absurd\", raising a number of questions. For example:\n",
    "\n",
    "- Do we really expect the 1 bat, 1 hit player to have such a high estimated batting average?\n",
    "- Do we really expect the 1 bat, 0 hits player to have such a low estimated batting average?\n",
    "- Don't we usually expect human performance to be approximately symmetrically distributed around some population mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Hierarchical Model\n",
    "\n",
    "With a hierarchical model, we can encode a set of assumptions into the model that may help us address the above questions. By a hierarchical model, we mean that each group's key parameter (in this case, the ${\\theta}$) have a parental distribution placed on top of them. This is sometimes anthropomorphically called \"sharing information\" between the distributions. (Not saying that this is a good or bad thing, just naming it as it is.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model diagram looks something like this:\n",
    "\n",
    "![](../images/baseball-hierarchical-model.jpg)\n",
    "\n",
    "Let's start by specifying the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as baseball_model:\n",
    "    \n",
    "    phi = pm.Uniform('phi', lower=0.0, upper=1.0)\n",
    "    kappa_log = pm.Exponential('kappa_log', lam=1.5)\n",
    "    kappa = pm.Deterministic('kappa', tt.exp(kappa_log))\n",
    "\n",
    "    thetas = pm.Beta('thetas', alpha=phi*kappa, beta=(1.0-phi)*kappa, shape=len(df))\n",
    "    like = pm.Binomial('like', n=df['AB'], p=thetas, observed=df['H'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we sample from the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with baseball_model:\n",
    "    trace = pm.sample(2000, njobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's inspect the traces for convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence looks good! Let's also look at the posterior distribution of batting averages per player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylabels = 'AB: ' + df['AB'].astype(str) + ', H: ' + df['H'].astype('str')\n",
    "pm.forestplot(trace, varnames=['thetas'], ylabels=ylabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears to me that these estimates are going to be much more reasonable. No player has a wildly high or low estimate on the basis of a single (or very few) data points.\n",
    "\n",
    "With a hierarchical model, we make the assumption that our observations (or treatments that group our observations) are somehow related. Under this assumption, when we have a new sample for which we have very few observations, we are able to borrow power from the population to make inferences about the new sample.\n",
    "\n",
    "Depending on the scenario, this assumption can either be reasonable, thereby not necessitating much debate, or be considered a \"strong assumption\", thereby requiring strong justification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrinkage\n",
    "\n",
    "\"Shrinkage\" is a term used to describe how hierarchical model estimation will usually result in parameter estimates that are \"shrunk\" away from their maximum likelihood estimators (i.e. the naive estimate from the data) towards the global mean.\n",
    "\n",
    "Shrinkage in and of itself is not necessarily a good or bad thing. However, because hierarchical models can sometimes be tricky to get right, we can use a shrinkage plot as a visual diagnostic for whether we have implemented the model correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE per player\n",
    "mle = df['batting_avg'].values\n",
    "# Non-hierarchical model\n",
    "no_pool = baseline_trace['thetas'].mean(axis=0)\n",
    "# Hierarchical model\n",
    "partial_pool = trace['thetas'].mean(axis=0)\n",
    "# MLE over all players\n",
    "complete_pool = np.array([df['batting_avg'].mean()] * len(df))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for r in np.vstack([mle, no_pool, partial_pool, complete_pool]).T:\n",
    "    ax.plot(r)\n",
    "    \n",
    "ax.set_xticks([0, 1, 2, 3])\n",
    "ax.set_xticklabels(['maximum\\nlikelihood', 'no\\npooling', 'partial\\npooling', 'complete\\npooling'])\n",
    "ax.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this shrinkage plot, as we go from the maximum likelihood estimator (MLE) (essentially not explicitly specifying priors) to a no-pooling model (with weak priors), there is \"shrinkage\" of the estimates towards the population mean (complete pooling). Incorporating a parental prior on each group's parameters further constrains the credible range of parameters. This, then, is the phenomenon of \"shrinkage\" in modelling.\n",
    "\n",
    "Just to reiterate again -- there is nothing ineherently right or wrong about shrinkage. Whether this is reasonable or not depends on our prior information about the problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian-modelling-tutorial",
   "language": "python",
   "name": "bayesian-modelling-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
