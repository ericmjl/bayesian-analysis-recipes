{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- author: Eric J. Ma\n",
    "- date: 16 March 2019\n",
    "---\n",
    "\n",
    "# Introduction\n",
    "\n",
    "In some datasets, we have truncated or qualified values. These may arise because of a few reasons:\n",
    "\n",
    "1. One might be that the measurement device has an upper- or lower-bound limit of detection\n",
    "2. Another might be because of protocol reasons, such as not subjecting an animal to a condition beyond a pre-defined \"ethical\" limit.\n",
    "\n",
    "This results in data for which a subset of values are real-valued, but the complementary set of values are imputed as the upper-bound or lower-bound value. \n",
    "\n",
    "For machine learning purposes, how do we deal with these bounds? One approach might be to approach it as a two-stage ML problem:\n",
    "\n",
    "1. In the first stage, predict whether the value is beyond our bounds or not.\n",
    "2. In the second stage, predict the actual value for those real-valued measurements. \n",
    "\n",
    "However, this means we lose the rich information stored in real-valued numbers. Perhaps there could be another way of approaching the problem?\n",
    "\n",
    "## Qualified Imputation\n",
    "\n",
    "In this notebook, I want to explore what imputation of qualified values would look like. In particular, I am choosing a parametric strategy, in which I impose a prior distribution on the data, optimize the parameters of the distribution to best fit the data, and finally draw numbers from that distribution to impute such that we remain in a regression setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as npr \n",
    "import numpy as np\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.stats import norm\n",
    "import pymc3 as pm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data\n",
    "\n",
    "First off, let's start with simulated data drawn from a standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=100, n_informative=10, n_targets=1)\n",
    "y = y / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now truncate the data such that any value above 2 is set to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncation_point = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trunc = np.concatenate(\n",
    "    [\n",
    "        np.clip(y, a_min=None, a_max=truncation_point),\n",
    "    ]\n",
    ")\n",
    "plt.hist(data_trunc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `data_trunc` represents our actual measured data. The data come from a standard Normal distribution, but there's a truncation point and hence an inflation of points at the truncation point, which is usually known. At the same time, there may be an inflation of values at the truncation point, for which we can model this using a mixture model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring Distributional Parameters\n",
    "\n",
    "Let us see if we are able to infer the standard Normal distribution parameters from `data_trunc`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model1:\n",
    "    model1.name = 'Normal'    \n",
    "    mu = pm.Normal('mu', 0, 10)\n",
    "    sig = pm.HalfNormal('sig', 10)\n",
    "    comp_dists = [\n",
    "        pm.Bound(pm.Normal, upper=truncation_point).dist(mu=mu, sd=sig), \n",
    "        pm.Constant.dist(truncation_point),\n",
    "    ]\n",
    "    \n",
    "    p = pm.Beta('p', alpha=1, beta=1)\n",
    "    weights = [p, 1-p]\n",
    "    like = pm.Mixture('like', w=weights, comp_dists=comp_dists, observed=data_trunc)\n",
    "    \n",
    "with model1:\n",
    "    trace1 = pm.sample(2000, njobs=1)\n",
    "    \n",
    "pm.traceplot(trace1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, with a very obvious spike in the upper truncated values, we are able to accurately recover `mu`, are very close to `sigma`, and are very accurate with `p`, the proportion of values drawn from the truncated standard Normal distribution.\n",
    "\n",
    "If we tried a different distribution with `(-inf, +inf)` support, we can check other distributions and perform model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model2:\n",
    "    model2.name = 'StudentT'\n",
    "    mu = pm.Normal('mu', 0, 10)\n",
    "    sig = pm.HalfNormal('sig', 5)\n",
    "    nu = pm.Exponential('nu', lam=1/29.)\n",
    "    comp_dists = [\n",
    "        pm.Bound(pm.StudentT, upper=truncation_point).dist(mu=mu, sd=sig, nu=nu), \n",
    "        pm.Constant.dist(truncation_point),\n",
    "    ]\n",
    "    \n",
    "    p = pm.Beta('p', alpha=1, beta=1)\n",
    "    weights = [p, 1-p]\n",
    "    like = pm.Mixture('like', w=weights, comp_dists=comp_dists, observed=data_trunc)\n",
    "    trace2 = pm.sample(2000, njobs=1)\n",
    "    \n",
    "pm.traceplot(trace2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.compare(\n",
    "    {\n",
    "        model1: trace1, \n",
    "        model2: trace2\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through model comparison, we see that the Normal distribution WAIC is lower than the StudentT distribution, but of course only marginally. For all practical purposes, I would adjudicate both to be equally good. Because the Normal distribution has fewer parameters to worry about, I would select that as the imputation distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import truncnorm\n",
    "\n",
    "impute_mu = trace1['Normal_mu'].mean()\n",
    "impute_sig = trace1['Normal_sig'].mean()\n",
    "\n",
    "impute_mu, impute_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set b to some exceedingly high number that is very unlikely, because truncnorm.rvs requires\n",
    "# both lower (a) and upper (b) to be specified.\n",
    "imputed_values = truncnorm.rvs(\n",
    "    a=truncation_point, \n",
    "    b=truncation_point*20, \n",
    "    loc=impute_mu, \n",
    "    scale=impute_sig,\n",
    "    size=len(data_trunc[data_trunc == truncation_point])\n",
    ")\n",
    "\n",
    "plt.hist(imputed_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new array with imputed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs, = np.where(data_trunc == truncation_point)\n",
    "data_imp = np.copy(data_trunc)\n",
    "np.put(data_imp, idxs, imputed_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf_scatter(data, ax, kwargs):\n",
    "    x, y = np.sort(data), np.arange(1, len(data)+1) / len(data)\n",
    "    ax.scatter(x, y, **kwargs)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ecdf_scatter(y, ax, dict(label='original'))\n",
    "ecdf_scatter(data_imp, ax, dict(label='imputed'))\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't recover the exact distribution, but something close enough for imputation purposes.\n",
    "\n",
    "Now, we can try our machine learning task on the imputed data, and compare how a model performs seeing imputed vs. original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_explained(est, X, y):\n",
    "    preds = est.predict(X)\n",
    "    return 1 - np.var(preds.ravel() - y.ravel()) / np.var(y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(n_estimators=1000)\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5)\n",
    "scores_ori = cross_val_score(rfr, X, y, cv=cv, scoring=variance_explained, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_imp = cross_val_score(rfr, X, data_imp, cv=cv, scoring=variance_explained, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "scores_df = pd.DataFrame(\n",
    "    {\n",
    "        'original': scores_ori,\n",
    "        'imputed': scores_imp,\n",
    "    }\n",
    ").melt(var_name='data', value_name='model_variance_explained')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(data=scores_df, x='data', y='model_variance_explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variance explained of 0.7 means that the residuals were only about 30% of the range of the data. For a model involving synthetic data, that is poor, but for biological data, that is pretty darn amazing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a practical setting where any predicted numbers above the truncated values are meaningful only in the sense that \"beyond truncation point\" == \"one meaning\", this method allows us to stay within the regression context. \n",
    "\n",
    "One has to take care when interpreting prediction values beyond the truncation point. In light of imputation, any predicted values beyond the imputation point can only be treated as \"possibly beyond the imputation point\", and they carry the same meaning as any other data point beyond the truncation point - \"high\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian",
   "language": "python",
   "name": "bayesian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
