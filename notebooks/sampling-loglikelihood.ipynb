{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, I want to take a deep dive into sampling from a log likelihood with vanilla MCMC. This will help me build my intuition around Bayesian statistics and more importantly, how to design a probabilistic programming API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap Bayes' Rule\n",
    "\n",
    "Bayes' rule is as follows:\n",
    "\n",
    "$$P(H|D)= \\frac{P(D|H)P(H)}{P(D)}$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $P(H)$ is the prior distribution on $H$ having not seen the data.\n",
    "- $P(H|D)$ is the posterior belief on $H$ having seen the data $D$.\n",
    "- $P(D|H)$ is the likelihood of the data.\n",
    "- $P(D)$ is the normalizing constant, or the probability of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating $P(X)$s into code\n",
    "\n",
    "I find I don't _truly_ understand something until I am able to translate it into code. Here, I will attempt to do so.\n",
    "\n",
    "I will start with the components that I know. \n",
    "\n",
    "Firstly, I know how to calculate the likelihood of data given a hypothesis. This involves writing a function that takes in data points, and calculates the sum of log likelihoods (or product of likelihoods) assuming that the samples of data are i.i.d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_loglike(x, **norm_params):\n",
    "    dist = norm(**norm_params)\n",
    "    \n",
    "    return np.sum(dist.logpdf(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use some fake data to test our understanding.\n",
    "\n",
    "I will generate fake data from a $N(3,1)$ distribution, but calculate the log likelihood under a $N(0,1)$ distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = norm(loc=3, scale=1).rvs(100)\n",
    "\n",
    "norm_loglike(xs, loc=0, scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compare that with the log likelihood from the actual distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_loglike(xs, loc=3, scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we still haven't had any \"priors\" injected here. Let's try it out with priors placed on the location of the Normal distribution, $m$.\n",
    "\n",
    "Assume $m \\sim N(0,10)$. In probability notation, \n",
    "\n",
    "$$P(m) = \\frac{1}{\\sigma_m \\sqrt{2 \\pi}} e^\\frac{-(m-\\mu_m)}{2 \\sigma_m^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglike_prior(m):\n",
    "    dist = norm(loc=0, scale=10)\n",
    "    return np.sum(dist.logpdf(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test a few prior values, and calculate their log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loglike_prior(0))\n",
    "print(loglike_prior(-1))\n",
    "print(loglike_prior(1))\n",
    "print(loglike_prior(-100))\n",
    "print(loglike_prior(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically what we would expect. $0$ has the highest log likelihood, and log-likelihood goes down symmetrically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, however, we need to use this prior in conjunction with the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_log_prob(m, xs):\n",
    "    return loglike_prior(m) + norm_loglike(xs, loc=m, scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we multiply the prior probability distribution by the likelihood probability distribution, we are doing nothing more than summing up their log likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lls = []\n",
    "for m in np.linspace(-10, 10, 100):\n",
    "    ll = joint_log_prob(m, xs)\n",
    "    lls.append(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-10, 10, 100), lls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(-10, 10, 100)[lls.index(max(lls))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo, this is close to the true value!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Version\n",
    "\n",
    "Let's now do it by _sampling_.\n",
    "\n",
    "We are going to use the [Metropolis-Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm).\n",
    "\n",
    "Briefly, it works as follows:\n",
    "\n",
    "- Initialize an arbitrary point.\n",
    "- Choose density to propose new point (we will use $N(m_{t-1}, 1)$).\n",
    "- For each iteration:\n",
    "    - Generate candidate new candidate $m_t$.\n",
    "    - Calculate acceptance ratio. We take advantage of the joint log probability L, by passing in the proposed value of $m_t$ into the logprob function, i.e. $L(m_t)$ and comparing it to $L(m_{t-1})$.\n",
    "    - Now, we compute the ratio $r = \\frac{L(m_t)}{L(m_{t-1})}$. \n",
    "    - Generate a new random number on the interval $p \\sim U(0, 1)$.\n",
    "    - Compare $p$ to $r$. \n",
    "        - If $p \\leq r$, accept $m_t$.\n",
    "        - If $p \\gt r$, reject $m_t$ and continue sampling again with $m_{t-1}$.\n",
    "        \n",
    "Let's write it in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metropolis-Hastings Sampling\n",
    "m_prev = np.random.normal(0, 1)\n",
    "\n",
    "\n",
    "history = dict()\n",
    "ratio_history = dict()\n",
    "for i in range(1000):\n",
    "    history[i] = m_prev\n",
    "    m_t = np.random.normal(m_prev, 5)\n",
    "    \n",
    "    # Compute negative joint log likelihood\n",
    "    L_t = np.exp(joint_log_prob(m_t, xs))\n",
    "    L_prev = np.exp(joint_log_prob(m_prev, xs))\n",
    "    \n",
    "    ratio = L_t / L_prev\n",
    "    ratio_history[i] = L_t\n",
    "    p = np.random.uniform(0, 1)\n",
    "    if p <= ratio:\n",
    "        m_prev = m_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([i for i in history.values()]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked!\n",
    "\n",
    "Now lies a challenge: How do we design an API that can handle not just `m` as the only variable, but arbitrary numbers of variables that have to be learned?\n",
    "\n",
    "Let's try this out with a model that has two random variables instead of one. Here, we'll add variance of the likelihood to the list of RVs.\n",
    "\n",
    "Here:\n",
    "\n",
    "$$\\mu \\sim N(0, 10)$$\n",
    "$$\\sigma \\sim Exp(2)$$\n",
    "$$Y \\sim N(\\mu, \\sigma)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "\n",
    "def normal_prior_loglike(mu):\n",
    "    return np.sum(norm(0, 10).logpdf(mu))\n",
    "\n",
    "def sigma_prior_loglike(sigma):\n",
    "    return np.sum(expon(scale=2).logpdf(sigma))\n",
    "\n",
    "def data_loglike(mu, sigma, xs):\n",
    "    return np.sum(norm(mu, sigma).logpdf(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loglike(mu, sigma, xs):\n",
    "    return normal_prior_loglike(mu) + sigma_prior_loglike(sigma) + data_loglike(mu, sigma, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metropolis-Hastings Sampling\n",
    "mu_prev = np.random.normal(0, 1)\n",
    "sigma_prev = np.random.normal(0, 1)\n",
    "\n",
    "mu_history = dict()\n",
    "sigma_history = dict()\n",
    "ratio_history = dict()\n",
    "\n",
    "# Let's test this with 100 steps.\n",
    "for i in range(100):\n",
    "    mu_history[i] = mu_prev\n",
    "    sigma_history[i] = sigma_prev\n",
    "    mu_t = np.random.normal(mu_prev, 5)\n",
    "    sigma_t = np.random.normal(sigma_prev, 5)\n",
    "    \n",
    "    # Compute negative joint log likelihood\n",
    "    LL_t = model_loglike(mu_t, sigma_t, xs)\n",
    "    LL_prev = model_loglike(mu_prev, sigma_prev, xs)\n",
    "    \n",
    "    ratio = np.exp(LL_t - LL_prev)\n",
    "    ratio_history[i] = ratio\n",
    "    p = np.random.uniform(0, 1)\n",
    "    if p <= ratio:\n",
    "        m_prev = m_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no, we get NaNs and infs in there! Why? Because we passed in invalid values into the logpdf of sigma. \n",
    "\n",
    "Sigma is bounded as a positive distribution, so we need to to somehow either:\n",
    "\n",
    "1. Restrict sigma's proposal distribution to be positive, or\n",
    "1. Transform sigma such that it lives on the real line, propose a new value on the real line, and then transform it back to original sigma.\n",
    "\n",
    "Let's try the first strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metropolis-Hastings Sampling\n",
    "mu_prev = np.random.normal(0, 1)\n",
    "sigma_prev = np.random.exponential(scale=1)\n",
    "\n",
    "mu_history = dict()\n",
    "sigma_history = dict()\n",
    "ratio_history = dict()\n",
    "\n",
    "for i in range(10000):\n",
    "    mu_history[i] = mu_prev\n",
    "    sigma_history[i] = sigma_prev\n",
    "    mu_t = np.random.normal(mu_prev, 0.1)\n",
    "    sigma_t = abs(np.random.normal(sigma_prev, 0.1))\n",
    "    \n",
    "    # Compute negative joint log likelihood\n",
    "    LL_t = model_loglike(mu_t, sigma_t, xs)\n",
    "    LL_prev = model_loglike(mu_prev, sigma_prev, xs)\n",
    "    \n",
    "    \n",
    "    # Calculate the ratio from the difference in log-likelihoods\n",
    "    # (or a.k.a. ratio of likelihoods)\n",
    "    diff_log_like = LL_t - LL_prev\n",
    "    if diff_log_like > 0:\n",
    "        ratio = 1\n",
    "    else:\n",
    "        ratio = np.exp(diff_log_like)\n",
    "    if np.isinf(ratio) or np.isnan(ratio):\n",
    "        raise ValueError(f\"LL_t: {LL_t}, LL_prev: {LL_prev}\")\n",
    "                \n",
    "    ratio_history[i] = ratio\n",
    "    p = np.random.uniform(0, 1)\n",
    "    \n",
    "    if ratio >= p:\n",
    "        mu_prev = mu_t\n",
    "        sigma_prev = sigma_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sigma_history.values(), label=\"sigma\")\n",
    "plt.hist(mu_history.values(), label=\"mu\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(sigma_history.values())).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(mu_history.values())).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo! It works!\n",
    "\n",
    "Now, let's try the second option, where we propose a number in real space, then transform it into the space of the distribution support, and then evaluate the new proposed sample.\n",
    "\n",
    "Doing so lets us use \"standard\" code to propose new distributions.\n",
    "\n",
    "With a positive distribution ($X > 0$), a logical transform for the distribution is the log-transform, as it is simple, invertible, and won't have invalid values ($ln(0)$ is impossible here).\n",
    "\n",
    "Because of this, the inverse of the transformation is going to be an exponential. Hence, we will sample in unconstrained, transformed space, use the inverse transform to go back to support space, and evaluate log likelihoods in support space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metropolis-Hastings Sampling\n",
    "mu_prev = np.random.normal(0, 1)\n",
    "sigma_prev = np.random.normal(0, 1)\n",
    "\n",
    "mu_history = dict()\n",
    "sigma_history = dict()\n",
    "ratio_history = dict()\n",
    "\n",
    "for i in range(1000):\n",
    "    mu_t = np.random.normal(mu_prev, 0.1)\n",
    "    # We'll make the changes here.\n",
    "    # Firstly, we sample from the normal distribution.\n",
    "    sigma_t = np.random.normal(sigma_prev, 0.1)\n",
    "    # Then, we transform the distribution by its inverse transformation.\n",
    "    # We will have to find a way to encapsulate this routine with a class method or function.\n",
    "    sigma_t_tfm = np.exp(sigma_t)\n",
    "    sigma_prev_tfm = np.exp(sigma_prev)\n",
    "\n",
    "    # Record history\n",
    "    mu_history[i] = mu_prev\n",
    "    sigma_history[i] = sigma_prev_tfm\n",
    "\n",
    "    # Compute negative joint log likelihood\n",
    "    LL_t = model_loglike(mu_t, sigma_t_tfm, xs)\n",
    "    LL_prev = model_loglike(mu_prev, sigma_prev_tfm, xs)\n",
    "\n",
    "    # Calculate the ratio from the difference in log-likelihoods\n",
    "    # (or a.k.a. ratio of likelihoods)\n",
    "    diff_log_like = LL_t - LL_prev\n",
    "    if diff_log_like > 0:\n",
    "        ratio = 1\n",
    "    else:\n",
    "        ratio = np.exp(diff_log_like)\n",
    "    if np.isinf(ratio) or np.isnan(ratio):\n",
    "        raise ValueError(f\"LL_t: {LL_t}, LL_prev: {LL_prev}\")\n",
    "\n",
    "    ratio_history[i] = ratio\n",
    "    p = np.random.uniform(0, 1)\n",
    "\n",
    "    if ratio >= p:\n",
    "        mu_prev = mu_t\n",
    "        sigma_prev = sigma_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list(sigma_history.values()), label=\"sigma\")\n",
    "plt.hist(list(mu_history.values()), label=\"mu\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the long tail of invalid values - that comes from the MCMC sampler stepping on its way to the region where the log likelihood is largest, but yet still isn't there. If we plot the histogram slightly differently, we will see what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(sigma_history)), list(sigma_history.values()))\n",
    "plt.plot(range(len(mu_history)), list(mu_history.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, there's a bit of a \"burn-in\" that the MCMC sampler needs before it reaches the region of \"optimal\" sampling. From manually observing the MCMC trace, we can see that sampling stabilizes around the correct values after 200+ steps. Let's set 200 to be the boundary at which we start plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list(sigma_history.values())[200:], label=\"sigma\")\n",
    "plt.hist(list(mu_history.values())[200:], label=\"mu\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks much better now! Notice how we've also recovered back the correct values, with associated uncertainty modelled in as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing an API for Sampling\n",
    "\n",
    "Now, the code for Metropolis-Hastings sampling has been copied many times over at this point, so it's about time to design an API that can handle arbitrary numbers of samples and arbitrary numbers of scalar random variables.\n",
    "\n",
    "Here are the design notes that we need to take care of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized API\n",
    "\n",
    "Firstly, the sampler API has to be consistent. A starter design might be to accept a model+data log-likelihood and its associated parameters, though thinking further about it, one might want to instead accept a \"container\" of sorts that wraps everything together (i.e. the data + model parameters + joint log likelihood)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Considerations\n",
    "\n",
    "Distributions need to have automatic transformations enabled.\n",
    "We need to be able to pass in a value in unconstrained, transformed space, but evaluate the log-likelihood in constrained, support space (wherever applicable). \n",
    "\n",
    "In PyMC3, we have context managers that handle things, but maybe a simpler way of handling this is to wrap distributions in a Python object, and provide the appropriate class methods, perhaps as follows:\n",
    "\n",
    "```python\n",
    "class Exponential(PositiveDistribution):\n",
    "    def sumlogpdf(x):\n",
    "        return np.sum(self.dist.logpdf(x))\n",
    "    \n",
    "    def sumlogpdf_tfm(x):\n",
    "        x = self.inverse_transform(x)\n",
    "        return self.logpdf(x)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In doing so, we could possibly avoid needing to explicitly specify proposal distributions and their transformations in the sampling loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distribution(object):\n",
    "    def __init__(self):\n",
    "        return NotImplementedError(\"Do not use abstract distribution class!\")\n",
    "\n",
    "    def transform(self, x):\n",
    "        \"\"\"Invertible transform that converts real number to distribution support.\"\"\"\n",
    "        return x\n",
    "    \n",
    "    def inv_transform(self, x):\n",
    "        \"\"\"Inverse of transform.\"\"\"\n",
    "        return x\n",
    "    \n",
    "    def sumlogpdf_tfm(self, x):\n",
    "        x = self.inv_transform(x)\n",
    "        return self.sumlogpdf(x)\n",
    "    \n",
    "    def sumlogpdf(self, x):\n",
    "        return np.sum(self.dist.logpdf(x))\n",
    "    \n",
    "    def sample(self, n):\n",
    "        return self.dist.rvs(n)\n",
    "\n",
    "    \n",
    "class PositiveDistribution(Distribution):\n",
    "    def transform(self, x):\n",
    "        \"\"\"Transformation for positive distributions is a log-transform.\"\"\"\n",
    "        return np.log(x)\n",
    "\n",
    "    def inv_transform(self, x):\n",
    "        return np.exp(x)\n",
    "\n",
    "\n",
    "class Normal(Distribution):\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dist = norm(loc=mu, scale=sigma)\n",
    "\n",
    "\n",
    "class Exponential(PositiveDistribution):\n",
    "    def __init__(self, lam):\n",
    "        self.lam = lam\n",
    "        self.dist = expon(scale=lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run some simple sanity checks to make sure everything was implemented correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we are going to calculate the log_prob of data for the exponential distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.exponential(scale=1.0, size=(10))\n",
    "\n",
    "X = Exponential(lam=1)\n",
    "X.sumlogpdf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try generating data from a normal distribution, and call on the transformed sumlogpdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(loc=0.0, scale=1.0, size=(10,))\n",
    "X = Exponential(lam=1)\n",
    "X.sumlogpdf_tfm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try generating data from a normal distribution, and call on the transformed and non-transformed sumlogpdf of the Normal distribution. The results should be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(loc=0, scale=1, size=10)\n",
    "X = Normal(mu=0, sigma=1)\n",
    "X.sumlogpdf(x), X.sumlogpdf_tfm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we're close to finishing the components needed in the distributions library of a PPL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another component we might need to worry about is that when we \"pass\" a distribution into another distribution as a parameter, what we are really doing is sampling a value from that distribution and then passing it into the next.\n",
    "\n",
    "As an example the following model in equations:\n",
    "\n",
    "$$\\mu \\sim N(0, 1)$$\n",
    "\n",
    "$$L \\sim N(\\mu, 1)$$\n",
    "\n",
    "translates to the following code (while simulating data from a model):\n",
    "\n",
    "```python\n",
    "mu = Normal(0, 1)\n",
    "L = Normal(mu.sample(1), 1)\n",
    "```\n",
    "\n",
    "However, while in inference mode, that is, when trying to sample posterior values of `mu`, we instead need the above code to be transformed into the following pseudocode:\n",
    "\n",
    "```python\n",
    "loglike_mu(mu) + loglike_L(mu, L, data)\n",
    "```\n",
    "\n",
    "\n",
    "TO BE CONTINUED..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to see whether we can use this to build a model that can return a log-probability function. That is basically what a model has to do.\n",
    "\n",
    "But first, some ideas to chew on.\n",
    "\n",
    "**Firstly**, the model has to know what its random variables, likelihood, and data are. It's possible to have more than one random variable, more than one likelihood, and hence more than one data source passed in. Knowing this implies that there is \"state\" involved, and hence a class-based implementation will make things easier to track than a function-based implementation.\n",
    "\n",
    "**Secondly**, the model has to know which random variables are passed into the likelihood function. Only then we can construct the evaluation of the likelihood correctly. To remind ourselves on why:\n",
    "\n",
    "```python\n",
    "def model_loglike(mu, sigma, xs):\n",
    "    return normal_prior_loglike(mu) + sigma_prior_loglike(sigma) + data_loglike(mu, sigma, xs)\n",
    "```\n",
    "\n",
    "Note here how `mu` has to be passed into the `normal_prior_loglike` and the `data_loglike`, while `sigma` has to be passed into the `sigma_prior_loglike` and the `data_loglike`. This induces a \"graph\" of sorts, which reveals to us another concept: Bayesian models have a \"graphical\" representation, through which data and model parameters flow through from the priors to the likelihood.\n",
    "\n",
    "**Thirdly**, for diagnostic purposes, we need to be able to do a \"prior sampling\" step, that is to say, simulate what data would look like if generated from our priors. In the absence of data, this step lets us calibrate our priors better: we could impose soft constraints (e.g. by tuning variance parameters) to ensure that the range of our data fall within the right order of mangitude(s) before fitting. We thus need to have an API that has something like `model.prior_samples()`, which then returns distributions for each of the latent parameters and simulated data from the model likelihood.\n",
    "\n",
    "**Fourthly**, again for diagnostic purposes, we need to be able to do a \"posterior sampling\" step, that is to simulate what the data looks like under the posterior sampled parameters. The steps involved here are to take each of the sampled posterior values of the random variables, pass them to the appropriate parts of the log-likelihood function, and generate data from the log-likelihood.\n",
    "\n",
    "**Fifthly**, we need the ability to do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to decompose this into its constituent components.\n",
    "\n",
    "1. We have a `sampler` to sample from the joint log-likelihood, which should accept proposed random variable values and data, and return a scalar.\n",
    "2. We have a `model` specification, which should accept data and return a log-likelihood function and the random variables values to evaluate.\n",
    "\n",
    "An API sketch might look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian",
   "language": "python",
   "name": "bayesian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
